{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cd428c-f678-4f5a-8e12-f69cb655876c",
   "metadata": {},
   "source": [
    "# Amadeus Flights Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751186b-7c1f-4a09-9b3e-b5493e88c895",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868b2af-0e49-4e15-a541-8c62c16dcace",
   "metadata": {},
   "source": [
    "In this lesson, we'll build an ETL pipeline that repeatedly extracts flight data using the Amadeus API.  There are various use cases for something like this.  For example, you can imagine us setting up alerts for when certain trips go below a certain price.  Or perhaps we want to collect data to reverse engineer our owning pricing algorithm, and then from there can predict the price of flights in the future.  \n",
    "\n",
    "\n",
    "Ok, so for our ETL pipeline we'll use Python, the boto3 library, dockerized lambda functions, and airflow.  We'll use Python to hit our API and extract data and load it to S3 using boto.  And then we'll dockerize our tasks and depoy them to lambda functions.  From there, we'll use airflow to call each of these lambda functions with boto3.  \n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79142c3-1efa-4c47-8e9b-0651f4a8bbdc",
   "metadata": {},
   "source": [
    "## I. Building the lambda functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d74a6-fea7-4018-b9b1-ba6d886c226a",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f1fa1-6e75-4b1a-8cb7-8e16bf3df779",
   "metadata": {},
   "source": [
    "The first step is to register for the Amadeus API, which is how we'll pull down our flights data.  You can register for a new account [here](https://developers.amadeus.com/register).\n",
    "\n",
    "Click on `Register` in the top right corner, and click on create an account.\n",
    "\n",
    "> **Trouble registering?** If you have a problem registering, try changing your browser to incognito mode, and then registering again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d574841-19a8-4a39-a1ea-745ff6f8d5e4",
   "metadata": {},
   "source": [
    "> <img src=\"./register-amadeus.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b79b2-bbb4-472c-bdb7-c2cbe49eef68",
   "metadata": {},
   "source": [
    "Then after registering, check your email to activate your account.  Then, login with your username and password and [create a new app](https://developers.amadeus.com/my-apps).\n",
    "\n",
    "> You can also create a new app by going to `My Self-Service Workspace` in the dropdown to the right, and then clicking on `Create new app`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e003b-e9cb-4423-83fb-54b3fccb3e9a",
   "metadata": {},
   "source": [
    "> <img src=\"./self-service.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3552bb-0351-4ed9-90b9-d80ecaa10c7f",
   "metadata": {},
   "source": [
    "After creating your application, if you scroll down, you'll see the API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9de924-e593-4622-bd41-78dce5fa6cc1",
   "metadata": {},
   "source": [
    "<img src=\"./amadeus-keys.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c7ec6-13bc-4043-9f66-9a8d028ea4e8",
   "metadata": {},
   "source": [
    "From here, we'll need to use our `api_key` and `api_secret` to get our access token, and then we can use the access token for the api."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed85cc-1d37-49a8-af0f-32813bac3f3a",
   "metadata": {},
   "source": [
    "## 1. Extract and Load - Making API requests \n",
    "\n",
    "For following write your code in the `extract_and_load` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e5f82-44da-4304-a1d3-4292892f3161",
   "metadata": {},
   "source": [
    "If you look at the `AmadeusClient`, you'll see how we can accomplish this.\n",
    "\n",
    "* `get_access_token` - We filled in most of the get_access_token function for you, as it is pretty complicated.  Notice, that we are making a post request to retrieve the access token, and specifying the `client_id` and `client_secret`.  Now the client_id and client_secret come from the settings file, which imports environmental variables from the `.env` file.  Fill in the `.env` file now, and then run the first test in `tests/test_amadeus_client` to ensure that an access token is retrieved.\n",
    "\n",
    "* `search_flights`\n",
    "    The API documentation for Amadeus is not great.  But you can get a sense of how it works with the following.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "from datetime import datetime\n",
    "\n",
    "originLocationCode = 'NYC'\n",
    "destinationLocationCode = 'CHI'\n",
    "departureDate = datetime.today().strftime('%Y-%m-%d')\n",
    "adults = 1\n",
    "\n",
    "url = 'https://test.api.amadeus.com/v2/shopping/flight-offers?' \\\n",
    "        'originLocationCode=' + originLocationCode + \\\n",
    "        '&destinationLocationCode=' + destinationLocationCode + \\\n",
    "        '&departureDate=' + departureDate + '&adults=' + str(adults)\n",
    "\n",
    "headers = CaseInsensitiveDict()\n",
    "headers['Authorization'] = 'Bearer {{ACCESS_TOKEN}}'\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1340fda-f628-43a5-9852-224215fbb5ef",
   "metadata": {},
   "source": [
    "Ok, so after looking at the above, pass through the same query parameters, but use a `params` dictionary to do so.  When it works, the second test in `test_amadeus_client` should pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33daa8f-4a24-4019-96dd-772d0f81b227",
   "metadata": {},
   "source": [
    "### File Writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5eaf8c-7fb3-46fa-91ce-4711d2bdf9aa",
   "metadata": {},
   "source": [
    "Next up is to save the extensive json response from the above, into an S3 bucket.  So the first step is to create a bucket in s3.\n",
    "\n",
    "After creating the bucket in s3, we'll want a function that will generate a file name for us, for us to store our results, and then we can create a new file in our bucket, that has this data.\n",
    "\n",
    "* `generate_file_name`\n",
    "    * This will take in arguments of origin, destination, and departure date, and return the proper string.  By default, it should create the file in a directory called `raw/`.\n",
    "    * We want to ensure that the file name always has two digits for the month (eg. `'raw/NYC-CHI-2023-06-28.json'`) -- even if the input string only has one digit (eg. `'2023-6-28'`).  Accomplish this to get the second test to pass.\n",
    "\n",
    "* `search_and_upload`\n",
    "    * Next, we'll write the `search_and_upload` function.  This function should use the `AmadeusClient#search_flights` function to return the flight data of those of a provided `origin`, `destination`, and `departure_date`.  It should then upload the data to s3 (we provided a function in  `file_writer` that should help).  And should then read the data from that file (we provided a different function that should help).  Finally, it should return a dictionary with keys of and `{'file': '', 'flight_data': ''}`.  The dictionary's values should be the `file_name` we uploaded the data to, as well as the first three records of flight offers. \n",
    "    \n",
    "    * In addition to getting the test to pass, go to the s3 web console to confirm that the file is uploaded to the bucket under the `raw` folder.  If it isn't, please fix it. \n",
    "    \n",
    "### Lambda Function\n",
    "\n",
    "Ok, so now it's time to write our lambda function.  Essentially, we would like our lambda function to -- given an event argument that has an `origin`, `destination`, and `departure_date_str` as it's key value pairs, then searches Amadeus and uploads the data in a corresponding file.\n",
    "\n",
    "Get the corresponding test to pass.  To do so, you'll have to provide your bucket name in the test.\n",
    "\n",
    "Ok, so at this point, we should have a lambda function, that given an event with keys of `origin`, `destination`, and `departure_date` will search the amadeus api for corresponding flights and write the results to a file in s3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3970d-439c-47db-aad5-9218416f8504",
   "metadata": {},
   "source": [
    "* Discussion\n",
    "\n",
    "So the nice thing about the step above is that we have stored all of our returned data in S3.  So while we likely do not need all of that data initially, we can always retrieve it from our S3 buckets later on.\n",
    "\n",
    "As the next step, we'll select just the data we want from our S3 bucket, and load that selected data to a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ce446-f6cc-4164-9f36-8e98a6a8e19c",
   "metadata": {},
   "source": [
    "## 2. Transform and Load - Transforming our Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f14c8-e5d8-4488-963d-592f26873446",
   "metadata": {},
   "source": [
    "Ok, so now we can navigate to the `transform_and_load` folder.\n",
    "\n",
    "As you can see, there we have two files: `flight_adapter` and `flights_file_reader`.\n",
    "\n",
    "### Flight adapter\n",
    "\n",
    "The flight takes in json regarding a single flight, and then the `select_attributes` function, returns just a dictionary with the `departure_time`, `arrival_time`, `departure_airport` and `arrival_airport` information.\n",
    "\n",
    "Before getting there, write the `AmadeusFlightAdapter#segments` function.  A segment is each flight in a given (one way) trip.\n",
    "\n",
    "* `Flight#segments`\n",
    "    * Write a function called segments, which given the json regarding a single one way flight, returns a list of dictionaries of the segments.  You can see a sample flight in the `test_flight_adapter` file.  Get the corresponding test to pass.\n",
    "\n",
    "* `Flight#select_attributes`\n",
    "    * As mentioned this will return the departure and arrival time, departure .  \n",
    "\n",
    "### Filereader\n",
    "\n",
    "The file reader does two things.  It contains multiple functions which read from data from our s3 bucket.  It also has a `select_attributes` function.\n",
    "\n",
    "* `Filereader#select_attributes`  This function loops through our `flights_json` returning a dictionary for each corresponding flight.   \n",
    "\n",
    "* `Filereader#return_flights_from_bucket`  This function both reads data from our bucket and a corresponding file in that bucket, and then returns the corresponding list of dictionaries of selected attributes of flights in that bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf154ef-9e53-4bc9-b3e5-168d06f6b6d2",
   "metadata": {},
   "source": [
    "Ok, so now we have a function, `Filereader#return_flights_from_bucket` that reads data from a file in our bucket and returns a list of dictionaries, where each dictionary has selected attributes.\n",
    "\n",
    "The next step is to write a lambda function, where we will both pull and coerce our data, and then write the results to new file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55357f-3a6d-4026-8c63-307b4dfcc52c",
   "metadata": {},
   "source": [
    "### Lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c888f-36ac-4754-b033-bc99ccf536bf",
   "metadata": {},
   "source": [
    "* `lambda_handler`\n",
    "    * If you look at the `test_main.py` function, you can see how our `lambda_handler` works.  \n",
    "\n",
    "> You'll have to change the test so that the `bucket_name` corresponds to your bucket, and the `file_name` refers to your bucket.\n",
    "\n",
    "So with the lambda function, we'll use our event to pass in our `file_name_read` which is the file with our raw data to our lambda function.  Then the function should pull data from a specified file in the raw folder of the bucket, and write the transformed data to file of the same name, in the same bucket, but in a folder called `transformed`.  \n",
    "\n",
    "Go to s3 and confirm that the uploaded data is in the `transformed` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a59ab0d-4169-4cab-b4ba-665f721689ac",
   "metadata": {},
   "source": [
    "# II. Moving to the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315cebb1-a66b-4280-a564-88def1d2c2a7",
   "metadata": {},
   "source": [
    "So now we have two lambda functions:\n",
    "\n",
    "1. `extract_load` - Pulls data from our api and loads to the our bucket under the `raw` folder.\n",
    "2. `transform_load` - Pulls data from the `raw` folder in s3, transforms and writes data to the `transformed` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe24aa9-9c4a-44e2-a2bd-4f3ee775849e",
   "metadata": {},
   "source": [
    "Now it's time to dockerize our two lambda functions, and eventually we'll deploy these dockerized lambda functions using serverless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15988947-0e29-45c0-a249-77c6ab7e0ccc",
   "metadata": {},
   "source": [
    "### 1. Dockerizing Extract_load\n",
    "\n",
    "To dockerize our extract and load function we'll need to move the following files in our image:\n",
    "\n",
    "* `requirements.txt`\n",
    "* `src/` directory\n",
    "* `main.py`\n",
    "* `.env`\n",
    "* `settings.py`\n",
    "\n",
    "And we'll have to install the libraries in `requirements.txt`.  In the requirements.txt file we'll need the following libraries.\n",
    "\n",
    "```txt\n",
    "python-dotenv\n",
    "requests\n",
    "boto3\n",
    "```\n",
    "\n",
    "Remember that for our command argument we'll need the following: `CMD [\"main.lambda_handler\"]`.  This will specify that the `lambda_handler` should be triggered when our lambda function receives an event.  \n",
    "\n",
    "Ok so we initially set `python:3.9` as our base image.  This is so we can debug and test out our docker image before we deploy it to aws.  We can do so by booting up our docker container, bashing into the container, and then calling our lambda function.\n",
    "\n",
    "So begin by building the image.\n",
    "\n",
    "`docker build -t extract .`\n",
    "\n",
    "And then you can bash in to the docker image with something like the following:\n",
    "\n",
    "```bash\n",
    "docker run -v ~/.aws:/root/.aws -it extract bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f3547-3389-44f9-ae69-98647b462904",
   "metadata": {},
   "source": [
    "> Notice what the above does.  It not only allows us to connect to our container via bash, but it also bind mounts our aws credentials (located in the `~/.aws` directory) to the corresponding directory in our docker container.  This way we can use our boto3 library from there.\n",
    "\n",
    "Ok, once we connect to the docker image, we can run our `main.py` file, and call our lambda function passing through sample `event` and `context` dictionaries).  \n",
    "\n",
    "> Below are a couple you can use.  Make sure the date is in the future.\n",
    "\n",
    "```python\n",
    "# context = {}\n",
    "# event = {\"origin\": \"NYC\", \"destination\": \"CHI\", 'departure_date_str': \"2023-08-02\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b15b17-cc03-4d37-a8a8-31b25f5d552c",
   "metadata": {},
   "source": [
    "Ok, once you call the lambda function, you should see the results, like we do below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48710188-fe1b-44f0-a525-6ae6a55520bd",
   "metadata": {},
   "source": [
    "<img src=\"./results-lambda.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b94ef-7789-44e7-badf-9fb346a0cd86",
   "metadata": {},
   "source": [
    "When you confirm that it's working, make sure to change the base image to the one for lambda.  It should be the following if you have a mac m1 or m2 chip.\n",
    "\n",
    "```Dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.9-arm64\n",
    "```\n",
    "Otherwise it should be:\n",
    "\n",
    "```Dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.9\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b3153-7ada-4309-bf3f-e8212014a61a",
   "metadata": {},
   "source": [
    "### 2. Dockerizing Transform_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4ceea-436f-405f-a5be-85173f77cb56",
   "metadata": {},
   "source": [
    "Ok, next up is to dockerize transform_load.  For this, you'll follow similar steps as we did before.  For the `requirements.txt` file, you'll need the following pip packages.\n",
    "\n",
    "```\n",
    "requests==2.25.1\n",
    "pandas\n",
    "boto3\n",
    "```\n",
    "\n",
    "Build an image called `transform` -- again using python:3.9 as the base image.\n",
    "\n",
    "```bash\n",
    "docker build -t transform .\n",
    "```\n",
    "\n",
    "Then confirm that the lambda function works by calling the lambda.  We provided some sample arguments to pass into your lambda function.  \n",
    "\n",
    "```python\n",
    "# file_name = 'NYC-CHI-2023-07-06.json'\n",
    "# event = {'file_name_read': f'raw/{file_name}'}\n",
    "# context = {}\n",
    "# lambda_handler(event, context)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd11ba04-c533-4509-81fa-677517c9f6bb",
   "metadata": {},
   "source": [
    "> You'll have to update the file_name to match one in your bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c16997-ce9b-4444-8742-ac38fc3e2b50",
   "metadata": {},
   "source": [
    "Then confirm that the lambda function works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb435fc-708d-44be-a9a7-4ef826ab40ed",
   "metadata": {},
   "source": [
    "<img src=\"./response.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038fa93-c5ec-4a60-84fb-eac8514022f3",
   "metadata": {},
   "source": [
    "Finally change the base image so that it's for a lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434c4f8-4ee9-475f-955d-1792e5c74343",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.9-arm64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a028aee1-295f-4186-9812-4801e820b668",
   "metadata": {},
   "source": [
    "### Deploying the lambda functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa0081-f4a3-4ef2-b75d-b071c306667e",
   "metadata": {},
   "source": [
    "Before moving on confirm, that the base images in each of the dockerfiles are using the lambda base image and not Python:3.9.  Please just check.\n",
    "\n",
    "Ok, so now, we'll use serverless to deploy our lambda functions.  You can review how we use serverless with Docker with the lesson Serverless Triggers located [here](https://colab.research.google.com/github/data-engineering-jigsaw/serverless-readings/blob/main/serverless-lesson/1-serverless-triggers.ipynb).\n",
    "\n",
    "> Note, that we actually don't need to trigger a lambda function with an S3 file being uploaded, because we'll use airflow to trigger each of our lambda functions.\n",
    "\n",
    "Ok, fill in the `serverless.yaml` file.  \n",
    "\n",
    "> Review serverless and give it a shot, but if you get stuck for over half an hour or so, feel free to look at the serverless.yaml file provided in this `lesson` directory.\n",
    "\n",
    "Remember that to deploy your lambda functions with serverless, you can navigate to the directory with `serverless.yaml` and then run `sls deploy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef7428-ecfa-4f08-897f-78f8ae103d4b",
   "metadata": {},
   "source": [
    "* Testing our deployed lambda functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e87532-4fe3-498b-a4b5-5b4de7cfff0c",
   "metadata": {},
   "source": [
    "Once our lambda functions are deployed it's time to test each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b6637-a1f2-4172-95d3-b410d07f20e3",
   "metadata": {},
   "source": [
    "1. `extract_load`\n",
    "\n",
    "First, go to the aws console, find the extract_load lambda function (it is likely preceded with the app name), and then let's test that it works by passing through a test event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3cb9e-be6a-4a8d-8e73-7e1ffb0d7eae",
   "metadata": {},
   "source": [
    "Fill in some event json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f607094-223c-44e4-9b69-5b2aebbedfd4",
   "metadata": {},
   "source": [
    "> <img src=\"./event-json.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1f82d-5545-4e01-bc45-cef40f5126a9",
   "metadata": {},
   "source": [
    "And then run the test event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c1dd3-060f-4a2d-9b69-f0d9d61da862",
   "metadata": {},
   "source": [
    "<img src=\"./success-fn.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c483b-4be6-4e13-82b1-b9de69b2d11c",
   "metadata": {},
   "source": [
    "Finally, go to the bucket and download and look at the data to confirm that it the proper json was pulled and uploaded (a lot of things can go wrong in our pipeline, so it's good to see it for yourself)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c11b3-bb09-4fa7-8e92-94238588fbff",
   "metadata": {},
   "source": [
    "<img src=\"./download-json.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22a577-314e-411e-b55e-929b63602cba",
   "metadata": {},
   "source": [
    "2. `transform_load`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de9553-bc00-45fa-a965-2eb14fb71009",
   "metadata": {},
   "source": [
    "Ok, let's do the same thing for our other lambda function. Go to the lambda function in the web console, and pass through a test event -- something like the followig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cf94c-3886-4ff4-a4c4-8dc2588435cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"file_name_read\": \"raw/NYC-CHI-2023-08-02.json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ff54c-940a-4ce9-9e1b-9377c9962e85",
   "metadata": {},
   "source": [
    "> <img src=\"./test-event.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4dbc5-f9c3-4ece-943c-52d9d78c571d",
   "metadata": {},
   "source": [
    "And then run the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df161ed-8c97-4ea6-94db-ee6114a406ab",
   "metadata": {},
   "source": [
    "<img src=\"./result.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350132c-e55e-4e88-8049-c689b7f113d3",
   "metadata": {},
   "source": [
    "Finally, go to the corresponding file in the transformed folder and confirm the file is there, and that it has a transformed list of dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fa0ab-41c8-4b05-8ee9-886c0976eb22",
   "metadata": {},
   "source": [
    "# III. Deploying to Airflow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d9824-7246-4d07-b8ed-7872e2a351be",
   "metadata": {},
   "source": [
    "Ok, so now that we have our lambda functions deployed, and we have confirmed that they each individually work, our next step is to have airflow call each of our functions.\n",
    "\n",
    "Navigate to the `airflow` folder.\n",
    "\n",
    "We'll get airflow to call our functions in two steps.  First, we'll write functions that use boto to invoke our lambda functions.  Then, we'll have airflow use separate tasks to call these functions in sequence.\n",
    "\n",
    "Ok, so first we'll set up boto to call our lambda functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12adb03-4595-4623-93c3-da4dc74e5463",
   "metadata": {},
   "source": [
    "### 1. Invoking our lambda functions from boto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe4d510-930d-4b7e-953a-2931ceefbf9d",
   "metadata": {},
   "source": [
    "There, you will notice a file called `dags/lambda_caller`.  \n",
    "\n",
    "* extract_load\n",
    "\n",
    "Ok, so we have filled out the first function for you.  This function uses boto to invoke our lambda function.  \n",
    "\n",
    "> You will just have to update the `function_name` -- to the name of your lambda function you want invoked.\n",
    "\n",
    "```python\n",
    "def extract_load(origin, destination, departure_date_str):\n",
    "    event = {\"origin\": origin,\n",
    "              \"destination\": destination,\n",
    "                \"departure_date_str\": departure_date_str}\n",
    "    function_name = 'flights-app-dev-extract_load'\n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName=function_name,\n",
    "        InvocationType='Event',  \n",
    "        Payload=json.dumps(event)\n",
    "    )\n",
    "    return response['ResponseMetadata']['RequestId']\n",
    "```\n",
    "\n",
    "If you look at the function above, the key part is with `lambda_client.invoke`.  There, we call the lambda function by passing through the `FunctionName` and we also pass through an event.  \n",
    "\n",
    "**Remember**, you'll have to have the function name match the name of your lambda function.\n",
    "\n",
    "If you look at the bottom of the file, you can see that we have code to pass through certain arguments to our `extract_load` function, and these arguments are used to fill in the event data that is then passed to our lambda function.  \n",
    "\n",
    "So run the `lambda_caller.py` file:\n",
    "\n",
    "```bash\n",
    "python3 -i lambda_caller.py\n",
    "```\n",
    "\n",
    "And then confirm that the file was loaded in the s3 bucket and that the data is in there.\n",
    "\n",
    "> **Warning**: This invocation is extremely picky, and the interaction with the amadeus api is not yet understood by man or machine, so please confirm that both the file was created, AND that the data is in there.  Also, try to keep the `extract_load` function as is, except for an update to the function name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0eca4-da74-4a01-8b4b-2a8336f6f1a0",
   "metadata": {},
   "source": [
    "* Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135aa8b6-c8e5-4c1b-8c0b-7629964a7d7b",
   "metadata": {},
   "source": [
    "If there are issues, look at the cloudwatch logs of the lambda function to debug.  Notice that we return the `RequestId` from our function, which you should be able to use, along with the timestamp in the logs to debug from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250cd38c-8633-40fd-8965-7d66d5ab7c11",
   "metadata": {},
   "source": [
    "> <img src=\"./cloudwatch-log.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f70667-b626-4bf9-80ed-e0fe8bcbfa31",
   "metadata": {},
   "source": [
    "> If you go to the lambda function, and then `monitor`, you'll see the `cloudwatch logs`.  Notice the `Timestamp`, and `RequestId` followed by a link to the related logs of that invocation.  If you do not see the requestid returned by the function, you may have to wait and refresh. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91fe1a4-979c-4952-af50-fe27243535f9",
   "metadata": {},
   "source": [
    "2. `transform_load`\n",
    "\n",
    "This function, we'll let *you* fill in.  Notice that at the bottom of the `lambda_caller` file, we have code to call the `extract_load` function.  In the second to last line, we call the `generate_file_name` function, this way we can use that return value to pass to our transform_load lambda function, to pull the data and transform it.\n",
    "\n",
    "Fill out the function, taking in the correct argument, and then go to the s3 bucket `transformed` folder to confirm that it was invoked.  Remember the debugging techniques specified above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f924ec-02cf-40d4-b174-5b2b82e3d104",
   "metadata": {},
   "source": [
    "### 2. Setting up airflow to call our functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e30fb32-cc58-4fd0-aaec-b92808a49423",
   "metadata": {},
   "source": [
    "Ok, so now we can use airflow to call our functions.  Before doing so, because airflow is using docker, we'll need to make sure that our docker containers have access to our aws credentials.\n",
    "\n",
    "Remember, how we did this previously with a single docker container -- we had to bindmount the .aws directory from our computer into our container.  Here, with docker compose, we have to do something similar.\n",
    "\n",
    "Take at the volumes key in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b519e58-be09-4c81-8286-f176e423bb23",
   "metadata": {},
   "source": [
    "<img src=\"./docker-compose.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6c3b0-26cd-485b-ae09-04fb30b98481",
   "metadata": {},
   "source": [
    "1. Bindmount the credentials\n",
    "\n",
    "With the last line, we are bindmounting the .aws directory in our laptop to the `/usr/local/airflow/.aws` directory in the container.  Normally, we would place this in the home directory (the place you go to with `cd ~`), but airflow does not allow us to write to this location.  So instead we bindmount to `/usr/local/airflow`.\n",
    "\n",
    "Notice the `/Users/jeffreykatz` -- change that to match your home directory.  You can type `echo ~` in the terminal to see what that is.\n",
    "\n",
    "2. Specify the location of our credentials\n",
    "\n",
    "Ok, so AWS will typically look for credentials in the environment's home directory -- but as we mentioned above, this time we are writing to the `usr/local/airflow` directory.  So we have to tell AWS to look for the credentials there. If you look at the last two key value pairs under `environment`, you can see that we are setting two environmental variables: `AWS_CONFIG_FILE` and `AWS_SHARED_CREDENTIALS_FILE`, to tell AWS where to look for these files.  \n",
    "\n",
    "Make sure you update your docker-compose file with the above changes.\n",
    "\n",
    "Then you can ensure aws is reading your AWS credentials from airflow by sh-ing into the container, typing `python3` and then using boto to check the current user.  \n",
    "\n",
    "Let's show you how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277b42e-ec72-4fe1-a675-429435f0aaec",
   "metadata": {},
   "source": [
    "First, from the airflow folder, boot up docker compose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7127b5d-828d-4064-a6e2-55227b699a2c",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker compose up\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9fdaf3-7869-4bbe-9611-4c111bc9e35a",
   "metadata": {},
   "source": [
    "Then from a different tab, bash into the webserver.\n",
    "\n",
    "```bash\n",
    "docker compose exec airflow-webserver bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a06c4-6f00-4ee2-901b-148f96a484a1",
   "metadata": {},
   "source": [
    "And then type `python3` to open up a python console.  And from there, you can get the current user with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3423513-f6ee-4028-b7f6-db88b7670049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "iam_client = boto3.client('iam')\n",
    "response = iam_client.get_user()\n",
    "user_name = response['User']['UserName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de1230-2198-422c-8a27-211c8569f4a2",
   "metadata": {},
   "source": [
    "### 3. Writing our airflow code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6aca1-135d-421c-813d-b38a28f2915d",
   "metadata": {},
   "source": [
    "Ok, now it's time to write our airflow code.  Essentially, we'll want a dag called `etl_dag` that calls two tasks -- `extract_load_task`, followed by `transform_load_task`. \n",
    "\n",
    "The dag should have the tasks defined nested inside of them like so.\n",
    "\n",
    "```python\n",
    "default_args = {'start_date': days_ago(1)}\n",
    "\n",
    "@dag(schedule_interval='@once', default_args=default_args, catchup=False)\n",
    "def etl_dag():\n",
    "    \n",
    "    @task\n",
    "    def extract_load_task(origin, destination, departure_date_str):\n",
    "        pass\n",
    "    \n",
    "    @task\n",
    "    def transform_load_task(origin, destination, departure_date_str):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b6cc7-bdf0-43b8-96e4-f886ff89ae0b",
   "metadata": {},
   "source": [
    "Ok, so you can see how we set this up, at the end of our dag, we define a few arguments to pass through our task individually.  \n",
    "\n",
    "```python\n",
    "origin = \"NYC\" # PHI\n",
    "destination = \"CHI\"\n",
    "departure_date_str = \"2023-07-6\"\n",
    "data = extract_load_task(origin, destination, departure_date_str)\n",
    "result = transform_load_task(origin, destination, departure_date_str)\n",
    "```\n",
    "\n",
    "You should try out different dates and origins and destinations -- just be careful to make sure you keep the same date format (two digits for the month).\n",
    "\n",
    "Ok, now it's your turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb703a2e-66c5-4d3d-9b33-877d6635dff0",
   "metadata": {},
   "source": [
    "For our airflow dag, we want the following to occur.\n",
    "\n",
    "* `extract_load_task (origin, destination, departure_date_str)` \n",
    "    * This should invoke our `extract_load` function defined in the `lambda_caller` file.  In other words, it should invoke our lambda function, passing through the arguments.  \n",
    "    * Add some logging so that we can confirm the origin destination and departure arguments.  You can do so, by placing the following line in the task `logging.info(f'extracting data from: {origin}, {destination}, departure: {departure_date_str}')`\n",
    "    * Also log the lambda_invocation_id that is returned from our `extract_load` function. So you should log something like: `extract_load lambda id: 2a1dac9c-4e51-4388-ad7c`.\n",
    "\n",
    "* `transform_load_task(origin, destination, departure_date_str)`\n",
    "    * This also takes in arguments of origin, destination, and departure_date_str.  Here, we'll need to use these arguments to call our transform_load function, which will trigger our lambda.  But to do this, we'll need to specify the file name to read from.  So use the `generate_file_name` defined in the utils file to do this.  (You'll have to do some work to make sure you read the correct file).  \n",
    "    * Log both the file you are reading from, and also log the result id from the `transform_load` invocation. \n",
    "    \n",
    "Finally, in the dag, make sure the `extract_load_task` is triggered before the `transform_load_task`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3629e-bdd2-4b6b-9232-89682af00e20",
   "metadata": {},
   "source": [
    "If you boot up the webserver, you should see the `etl_dag`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf7938-1ac0-406f-a777-a01eb9c31355",
   "metadata": {},
   "source": [
    "<img src=\"./etl-dag.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b3ec8-bb92-4a72-aa61-0e235a04abf8",
   "metadata": {},
   "source": [
    "> If there is a red error above, click on it, read the error message and make the fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3901e-9017-4741-8ff3-e73acd01aedb",
   "metadata": {},
   "source": [
    "If you click on the dag, and go to graph view, you should be able to confirm that the `extract_load_task` comes before the `transform_load_task`.\n",
    "\n",
    "> There should be an arrow between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5da635-c6d1-41e7-8d88-04520ca8ba4f",
   "metadata": {},
   "source": [
    "<img src=\"./two-tasks.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe5a97-1cde-4525-97ef-696a95f90ca2",
   "metadata": {},
   "source": [
    "You can test out the dag by clicking on the `Trigger Dag` button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628dc58a-3e62-4799-9469-b60308f30eb0",
   "metadata": {},
   "source": [
    "<img src=\"./trigger-dag.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f823d0d-faf3-42a1-b3a6-4c668758f013",
   "metadata": {},
   "source": [
    "And then triggering the dag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6c266-d100-4e09-a378-b23b1828dcc3",
   "metadata": {},
   "source": [
    "Then let's check the log of each of the tasks.  Go to the graph view, and click on the extract_load_task box, followed by logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b01b42-739c-4dcc-81e9-cfc70e294204",
   "metadata": {},
   "source": [
    "<img src=\"./log-el-task.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d6ddd-2cd1-476d-9634-78d0bcd8f1df",
   "metadata": {},
   "source": [
    "If you scroll down in the logs, you should see the logged information we specified in the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c58245-30f3-4202-91e3-5a09c4ae3436",
   "metadata": {},
   "source": [
    "<img src=\"./logged-info.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce63820-bc39-4f24-baf4-496bbcdda370",
   "metadata": {},
   "source": [
    "Now check the logs of the transform load task.  You should see both the file we are reading from and the invocation id of the lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f430d9-77ff-450f-ad91-ceb41799ef91",
   "metadata": {},
   "source": [
    "<img src=\"./tl-log.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8f242-f161-49c4-9909-90f81e4a5779",
   "metadata": {},
   "source": [
    "Ok, it's not done until we confirm that we have our raw and transformed files in the bucket.  So go to s3 and confirm that the file is in our bucket, and click on the file to make sure that our json is in there, and that the flights are for the correct date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1419a57-8650-4a0e-ab75-dbe6a73d39d8",
   "metadata": {},
   "source": [
    "Ok, once you do that you can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca0656-69f6-4135-8f73-22b672be323e",
   "metadata": {},
   "source": [
    "* Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebacd40-ef12-4526-9e68-7f9e9bdd833a",
   "metadata": {},
   "source": [
    "Notice that our airflow code is very small.  All our airflow tasks do is trigger our lambda invocation functions.  By keeping our airflow functions small we get a number of benefits.\n",
    "\n",
    "1. Testing -- We cannot really write tests for the tasks.  This is a big deal.  The more logic we have inside of our tasks, the more of that logic will be untested.\n",
    "\n",
    "2. Debugging -- Related to that.  It's pretty difficult to debug inside of our airflow containers.  For example, we can't really place a `breakpoint()` from inside of our airflow docker container?  Also, if we do run into a bug it's would be difficult to know if it's related to airflow or specific to our code.\n",
    "\n",
    "3. Lockin - What if we want to move to a different orchestrator -- like prefect, or dagster?  If we choose to -- there's very little code to migrate.  \n",
    "\n",
    "4. Breakdown - We'll need to run airflow on an EC2 instance.  If that instance goes down, or we have trouble debugging it, we can just use event scheduler to trigger our lambda functions.\n",
    "\n",
    "So why use airflow at all?\n",
    "\n",
    "With airflow, we still get the benefit of logging.  We can use features like retrying the function if the lambda breaks, or emailing someone if the task breaks with configuration like the following.\n",
    "\n",
    "```python\n",
    "default_args = {\n",
    "    'owner': 'me',\n",
    "    'start_date': datetime(2019, 2, 8),\n",
    "    'email': ['you@work.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': True,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=1)\n",
    "}\n",
    "```\n",
    "\n",
    "So here, we get the benefit of airflow's logging, retries, and notifications (like sending an email).  But also have our lambda functions independently hosted. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73d790-accf-4b0a-9533-939902b15659",
   "metadata": {},
   "source": [
    "### What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903bdf4-e207-4458-8a56-ebe21f5e8a4c",
   "metadata": {},
   "source": [
    "1. Load to a database\n",
    "\n",
    "* Ideally, we would complete the cycle by loading the transformed data from our `transformed` folder into postgres, redshift, or snowflake.  If you look at the `load_to_postgres` folder, you can see some sample code for doing this.  You can see how we could use a snowflake connector to accomplish something similarly [here](https://docs.snowflake.com/en/developer-guide/python-connector/sqlalchemy).\n",
    "\n",
    "* In addition, if we can deploy a database with serverless, we should update our file to do so.\n",
    "\n",
    "2. Let's say we wanted to read a list of destinations from a database, and create a separate dag from each one.  You can get a sense of how we might do so with [Dag generation](https://airflow.apache.org/docs/apache-airflow/stable/howto/dynamic-dag-generation.html).\n",
    "\n",
    "3. Deploying airflow to AWS.  Also remember that we will need this orchestrator to live somewhere on aws.  You can get a sense of how to do so [here](https://www.youtube.com/watch?v=o88LNQDH2uI&ab_channel=DatawithMarc), but remember you'll have to migrate the aws credentials.  You can see that we can just directly set the keys with environmental variables as shown [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
